{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f5ec06-eec3-4c63-968e-63379ae50bb4",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80196e-4dae-46a9-8fd9-b2f7f5e4d617",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c760aa7-94b2-41df-a8f0-4bd1dc9092fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1eaf95-4364-46ad-bb74-333ce8d3b97e",
   "metadata": {},
   "source": [
    "## Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea378cc-7810-4a29-bb06-968dce21bb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b33a95-4b1a-4605-9de8-c786d5cd7b95",
   "metadata": {},
   "source": [
    "## Pre-process Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf8ac0a-dbc9-40e5-ac21-fb089ff64db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of all folders\n",
    "folders = '/home/jovyan/work/GitHub/convolutionalNeuralNet/'\n",
    "classes = [name for name in os.listdir(folders + 'images/temp') if os.path.isdir(os.path.join(folders + 'images/temp', name))]\n",
    "\n",
    "train_path = folders + 'images/train/'\n",
    "valid_path = folders + 'images/valid/'\n",
    "test_path = folders + 'images/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d7403c-72d5-4408-b7e8-21a22f8abca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Albania', 'Mexico', 'Kyrgyzstan', 'UnitedKingdom', 'UnitedStates']\n"
     ]
    }
   ],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aabca5c-5f0b-4a7a-a3c1-26817cff7f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 508 images belonging to 5 classes.\n",
      "Found 105 images belonging to 5 classes.\n",
      "Found 20 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Using the VGG16 image processing for RBG images\n",
    "# Preprocessing the training, validation, and testing images\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "train_batches = train_datagen.flow_from_directory(\n",
    "        folders + 'images/train',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=10,\n",
    "        class_mode='categorical')\n",
    "valid_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "valid_batches = valid_datagen.flow_from_directory(\n",
    "        folders + 'images/valid',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=10,\n",
    "        class_mode='categorical')\n",
    "test_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "test_batches = test_datagen.flow_from_directory(\n",
    "        folders + 'images/test',\n",
    "        target_size=(224, 224),\n",
    "        batch_size=10,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# train_batches = ImageDataGenerator(\n",
    "#     preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    "# ).flow_from_directory(\n",
    "#     directory=train_path,\n",
    "#     target_size=(224, 224),\n",
    "#     classes=classes,\n",
    "#     batch_size=10\n",
    "# )\n",
    "\n",
    "# valid_batches = ImageDataGenerator(\n",
    "#     preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    "# ).flow_from_directory(\n",
    "#     directory=valid_path,\n",
    "#     target_size=(224, 224),\n",
    "#     classes=classes,\n",
    "#     batch_size=10\n",
    "# )\n",
    "\n",
    "# test_batches = ImageDataGenerator(\n",
    "#     preprocessing_function=tf.keras.applications.vgg16.preprocess_input\n",
    "# ).flow_from_directory(\n",
    "#     directory=test_path,\n",
    "#     target_size=(224, 224),\n",
    "#     classes=classes,\n",
    "#     batch_size=10,\n",
    "#     shuffle=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34814da-4932-4996-8cc2-100dfad95e5c",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60939712-0497-4b9b-ad71-2ba544b04af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(                         # Hidden layer\n",
    "        filters=32,                 # Standard value\n",
    "        kernel_size=(3, 3),         # Standard value\n",
    "        activation='relu',          # Standard activation\n",
    "        padding='same',             # No padding\n",
    "        input_shape=(224, 224, 3)   # Shape of image\n",
    "    ),\n",
    "    MaxPool2D(                      # Max pooling for dimension reduction\n",
    "        pool_size=(2, 2),           # Filter size\n",
    "        strides=2                   # Cut in half\n",
    "    ),\n",
    "    Conv2D(                         # Hidden layer\n",
    "        filters=64,                 # Increase filters as you add layers\n",
    "        kernel_size=(3, 3),\n",
    "        activation='relu',\n",
    "        padding='same'\n",
    "    ),\n",
    "    MaxPool2D(                      # Max pooling for dimension reduction\n",
    "        pool_size=(2, 2),\n",
    "        strides=2\n",
    "    ),\n",
    "    # Conv2D(                         # Hidden layer\n",
    "    #     filters=128,\n",
    "    #     kernel_size=(3, 3),\n",
    "    #     activation='relu',\n",
    "    #     padding='same'\n",
    "    # ),\n",
    "    # MaxPool2D(                      # Max pooling for dimension reduction\n",
    "    #     pool_size=(2, 2),\n",
    "    #     strides=2\n",
    "    # ),\n",
    "    Flatten(),                      # Flatten for output layer\n",
    "    Dense(\n",
    "        units=128,\n",
    "        activation='relu'\n",
    "    ),\n",
    "    Dense(                          # Output layer\n",
    "        units=len(classes),\n",
    "        activation='softmax'\n",
    "    )\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b29cc-932c-4947-acff-efba74f5fcf7",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac4335d-5aa9-4c10-bf66-5f1dcdd703f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "51/51 [==============================] - 8s 127ms/step - loss: 1.6672 - accuracy: 0.2146 - val_loss: 1.4844 - val_accuracy: 0.3143\n",
      "Epoch 2/15\n",
      "51/51 [==============================] - 6s 116ms/step - loss: 1.5358 - accuracy: 0.3248 - val_loss: 1.4406 - val_accuracy: 0.3048\n",
      "Epoch 3/15\n",
      "51/51 [==============================] - 6s 116ms/step - loss: 1.5173 - accuracy: 0.3110 - val_loss: 1.4478 - val_accuracy: 0.3905\n",
      "Epoch 4/15\n",
      "51/51 [==============================] - 6s 117ms/step - loss: 1.4762 - accuracy: 0.3681 - val_loss: 1.4100 - val_accuracy: 0.4286\n",
      "Epoch 5/15\n",
      "51/51 [==============================] - 6s 116ms/step - loss: 1.4431 - accuracy: 0.3957 - val_loss: 1.4383 - val_accuracy: 0.3905\n",
      "Epoch 6/15\n",
      "51/51 [==============================] - 6s 116ms/step - loss: 1.4251 - accuracy: 0.3780 - val_loss: 1.4501 - val_accuracy: 0.3619\n",
      "Epoch 7/15\n",
      "51/51 [==============================] - 6s 117ms/step - loss: 1.4027 - accuracy: 0.3957 - val_loss: 1.3788 - val_accuracy: 0.4190\n",
      "Epoch 8/15\n",
      "51/51 [==============================] - 6s 116ms/step - loss: 1.3769 - accuracy: 0.4154 - val_loss: 1.4179 - val_accuracy: 0.4190\n",
      "Epoch 9/15\n",
      "51/51 [==============================] - 6s 117ms/step - loss: 1.3591 - accuracy: 0.4311 - val_loss: 1.4190 - val_accuracy: 0.4286\n",
      "Epoch 10/15\n",
      "51/51 [==============================] - 6s 117ms/step - loss: 1.3238 - accuracy: 0.4350 - val_loss: 1.4847 - val_accuracy: 0.3714\n",
      "Epoch 11/15\n",
      "51/51 [==============================] - 6s 117ms/step - loss: 1.2537 - accuracy: 0.4980 - val_loss: 1.5607 - val_accuracy: 0.4190\n",
      "Epoch 12/15\n",
      "51/51 [==============================] - 6s 117ms/step - loss: 1.2639 - accuracy: 0.4783 - val_loss: 1.3547 - val_accuracy: 0.4857\n",
      "Epoch 13/15\n",
      "51/51 [==============================] - 6s 116ms/step - loss: 1.2288 - accuracy: 0.5059 - val_loss: 1.3708 - val_accuracy: 0.4381\n",
      "Epoch 14/15\n",
      "51/51 [==============================] - 6s 117ms/step - loss: 1.1775 - accuracy: 0.5354 - val_loss: 1.3708 - val_accuracy: 0.4857\n",
      "Epoch 15/15\n",
      "51/51 [==============================] - 6s 118ms/step - loss: 1.1695 - accuracy: 0.5079 - val_loss: 1.3104 - val_accuracy: 0.4762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fac2dbfc940>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_batches,\n",
    "    validation_data=valid_batches,\n",
    "    epochs=15,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f019d-4c00-4994-a3ef-0dbe26222557",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe403aa-4384-427f-b659-38520165bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/jovyan/work/GitHub/convolutionalNeuralNet/models/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(folders + 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d78ecf-8808-4ea4-88b7-39bcf51b0a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
